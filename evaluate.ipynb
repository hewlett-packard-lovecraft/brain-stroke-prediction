{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# evaluation\n",
        "\n",
        "Evaluate both baseline and TabTransformer models with test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import keras_preprocessing, tensorflow_addons, keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.metrics.metrics.AUC at 0x7f6079e77a00>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_label(data: pd.DataFrame):\n",
        "    x = data.copy().drop('stroke', axis=1)\n",
        "    y = data[\"stroke\"]  # labels\n",
        "\n",
        "    return x, y\n",
        "\n",
        "keras.metrics.AUC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_HEADER = [\n",
        "    \"gender\",\n",
        "    \"age\",\n",
        "    \"hypertension\",\n",
        "    \"heart_disease\",\n",
        "    \"ever_married\",\n",
        "    \"work_type\",\n",
        "    \"residence_type\",\n",
        "    \"avg_glucose_level\",\n",
        "    \"bmi\",\n",
        "    \"smoking_status\",\n",
        "    \"stroke\",\n",
        "]\n",
        "\n",
        "FEATURES = CSV_HEADER[:-1]\n",
        "TARGET = CSV_HEADER[-1]\n",
        "\n",
        "test_data_path = Path().resolve().joinpath(\"dataset/test_data.csv\")\n",
        "test_data_file = str(test_data_path.absolute())\n",
        "test_data = pd.read_csv(test_data_file, names=CSV_HEADER)\n",
        "\n",
        "x_test, y_test = split_label(test_data)\n",
        "\n",
        "y_test = y_test.replace({\"No\": 0, 'Yes': 1})\n",
        "\n",
        "x_test = x_test\n",
        "y_test = y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "DROPOUT_RATE = 0.1\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "MLP_MODEL_PATH=str(Path().resolve().joinpath('model/mlp_model'))\n",
        "TABTRANSFORMER_MODEL_PATH=str(Path().resolve().joinpath('model/tabtransformer_model'))\n",
        "\n",
        "TARGET_FEATURE_NAME='stroke'\n",
        "TARGET_LABELS = [1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# data proccessing pipeline\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m target_label_lookup \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mStringLookup(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     vocabulary\u001b[39m=\u001b[39;49mTARGET_LABELS, mask_token\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, num_oov_indices\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_example\u001b[39m(features, target):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#target_index = target_label_lookup(target)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/evaluate.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     target_index \u001b[39m=\u001b[39m target\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/preprocessing/string_lookup.py:326\u001b[0m, in \u001b[0;36mStringLookup.__init__\u001b[0;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, idf_weights, encoding, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m   encoding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding \u001b[39m=\u001b[39m encoding\n\u001b[0;32m--> 326\u001b[0m \u001b[39msuper\u001b[39;49m(StringLookup, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    327\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m    328\u001b[0m     num_oov_indices\u001b[39m=\u001b[39;49mnum_oov_indices,\n\u001b[1;32m    329\u001b[0m     mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    330\u001b[0m     oov_token\u001b[39m=\u001b[39;49moov_token,\n\u001b[1;32m    331\u001b[0m     vocabulary\u001b[39m=\u001b[39;49mvocabulary,\n\u001b[1;32m    332\u001b[0m     vocabulary_dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mstring,\n\u001b[1;32m    333\u001b[0m     idf_weights\u001b[39m=\u001b[39;49midf_weights,\n\u001b[1;32m    334\u001b[0m     invert\u001b[39m=\u001b[39;49minvert,\n\u001b[1;32m    335\u001b[0m     output_mode\u001b[39m=\u001b[39;49moutput_mode,\n\u001b[1;32m    336\u001b[0m     sparse\u001b[39m=\u001b[39;49msparse,\n\u001b[1;32m    337\u001b[0m     pad_to_max_tokens\u001b[39m=\u001b[39;49mpad_to_max_tokens,\n\u001b[1;32m    338\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    339\u001b[0m base_preprocessing_layer\u001b[39m.\u001b[39mkeras_kpl_gauge\u001b[39m.\u001b[39mget_cell(\u001b[39m\"\u001b[39m\u001b[39mStringLookup\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mset(\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/preprocessing/index_lookup.py:289\u001b[0m, in \u001b[0;36mIndexLookup.__init__\u001b[0;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary_dtype, vocabulary, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf_weights_const \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf_weights\u001b[39m.\u001b[39mvalue()\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m vocabulary \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_vocabulary(vocabulary, idf_weights)\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m   \u001b[39m# When restoring from a keras SavedModel, the loading code will expect to\u001b[39;00m\n\u001b[1;32m    292\u001b[0m   \u001b[39m# find and restore a lookup_table attribute on the layer. This table needs\u001b[39;00m\n\u001b[1;32m    293\u001b[0m   \u001b[39m# to be uninitialized as a StaticHashTable cannot be initialized twice.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_uninitialized_lookup_table()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/preprocessing/index_lookup.py:485\u001b[0m, in \u001b[0;36mIndexLookup.set_vocabulary\u001b[0;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (new_vocab_size \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens):\n\u001b[1;32m    481\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mAttempted to set a vocabulary larger than the maximum vocab size. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mPassed vocab size is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, max vocab size is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    484\u001b[0m           new_vocab_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens))\n\u001b[0;32m--> 485\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lookup_table_from_tokens(tokens)\n\u001b[1;32m    487\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_mode \u001b[39m==\u001b[39m TF_IDF:\n\u001b[1;32m    488\u001b[0m   \u001b[39mif\u001b[39;00m idf_weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/preprocessing/index_lookup.py:704\u001b[0m, in \u001b[0;36mIndexLookup._lookup_table_from_tokens\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    702\u001b[0m indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrange(token_start, token_end, dtype\u001b[39m=\u001b[39mindices_dtype)\n\u001b[1;32m    703\u001b[0m keys, values \u001b[39m=\u001b[39m (indices, tokens) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minvert \u001b[39melse\u001b[39;00m (tokens, indices)\n\u001b[0;32m--> 704\u001b[0m initializer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlookup\u001b[39m.\u001b[39;49mKeyValueTensorInitializer(keys, values,\n\u001b[1;32m    705\u001b[0m                                                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key_dtype,\n\u001b[1;32m    706\u001b[0m                                                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value_dtype)\n\u001b[1;32m    707\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mlookup\u001b[39m.\u001b[39mStaticHashTable(initializer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_value)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/lookup_ops.py:547\u001b[0m, in \u001b[0;36mKeyValueTensorInitializer.__init__\u001b[0;34m(self, keys, values, key_dtype, value_dtype, name)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    545\u001b[0m         values, dtype\u001b[39m=\u001b[39mvalue_dtype, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(keys, dtype\u001b[39m=\u001b[39;49mkey_dtype, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mkeys\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    548\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    549\u001b[0m       values, dtype\u001b[39m=\u001b[39mvalue_dtype, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mkey_value_init\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1640\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1631\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1632\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1633\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1636\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1637\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1639\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1640\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1642\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1643\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
          ]
        }
      ],
      "source": [
        "# data proccessing pipeline\n",
        "\n",
        "target_label_lookup = layers.StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "\n",
        "\n",
        "def prepare_example(features, target):\n",
        "    #target_index = target_label_lookup(target)\n",
        "    target_index = target\n",
        "    return features, target_index\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
        "    \"\"\"dataset from, csv\"\"\"\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
        "    return dataset.cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evalate_model(model: keras.Model, test_data_file):\n",
        "    test_data = get_dataset_from_csv(test_data_file)\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(\n",
        "                num_thresholds=200,\n",
        "                curve=\"ROC\",\n",
        "                summation_method=\"interpolation\",\n",
        "                name=\"auc\",\n",
        "            ),\n",
        "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.evaluate(\n",
        "        x=test_data,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=\"auto\",\n",
        "        steps=None,\n",
        "        callbacks=None,\n",
        "        max_queue_size=10,\n",
        "        workers=1,\n",
        "        use_multiprocessing=False,\n",
        "        return_dict=False,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def predict_model(model: keras.Model, test_data_file):\n",
        "    test_data = get_dataset_from_csv(test_data_file)\n",
        "\n",
        "    return model.predict(\n",
        "        x=test_data,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=\"auto\",\n",
        "        steps=None,\n",
        "        callbacks=None,\n",
        "        max_queue_size=10,\n",
        "        workers=1,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 22ms/step - loss: 0.2088 - auc: 0.8179 - accuracy: 0.9397\n",
            "4/4 [==============================] - 1s 26ms/step - loss: 0.2443 - auc: 0.7288 - accuracy: 0.9293\n"
          ]
        }
      ],
      "source": [
        "baseline_model = keras.models.load_model(MLP_MODEL_PATH)\n",
        "tt_model = keras.models.load_model(TABTRANSFORMER_MODEL_PATH)\n",
        "\n",
        "evalate_model(baseline_model, test_data_file)\n",
        "evalate_model(tt_model, test_data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLP:\n",
        "loss: 0.2073 - auc: 0.8241 - accuracy: 0.9397\n",
        "### TabTransformer:\n",
        "loss: 0.2197 - auc: 0.7708 - accuracy: 0.9335"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5d9c7b03d18c7708763fb0db9310b2d7254dfc13fc51ca537e4e50c45d8de958"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
