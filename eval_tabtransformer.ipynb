{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# evaluation\n",
        "\n",
        "Evaluate both baseline and TabTransformer models with test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras_preprocessing\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import sklearn\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preproccessing\n",
        "\n",
        "CSV_HEADER = [\n",
        "    \"gender\",\n",
        "    \"age\",\n",
        "    \"hypertension\",\n",
        "    \"heart_disease\",\n",
        "    \"ever_married\",\n",
        "    \"work_type\",\n",
        "    \"residence_type\",\n",
        "    \"avg_glucose_level\",\n",
        "    \"bmi\",\n",
        "    \"smoking_status\",\n",
        "    \"stroke\",\n",
        "]\n",
        "\n",
        "\n",
        "CATEGORICAL_FEATURE_NAMES = [\n",
        "    \"gender\",\n",
        "    \"hypertension\",\n",
        "    \"heart_disease\",\n",
        "    \"ever_married\",\n",
        "    \"work_type\",\n",
        "    \"residence_type\",\n",
        "    \"smoking_status\",\n",
        "]\n",
        "\n",
        "NUMERIC_FEATURE_NAMES = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
        "\n",
        "\n",
        "def encode_int(data: pd.DataFrame, categorical_features: list[str]):\n",
        "    return pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "\n",
        "def split_label(data: pd.DataFrame):\n",
        "    x = data.copy().drop(\"stroke\", axis=1)\n",
        "    y = data[\"stroke\"]  # labels\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def resample(data: pd.DataFrame, seed: int, categorical_features: list[str]):\n",
        "    \"\"\"oversample positive cases with SMOTE and undersample negative with EEN\"\"\"\n",
        "    # encode categorical features first\n",
        "    enc = OrdinalEncoder()\n",
        "    data[categorical_features] = enc.fit_transform(data[categorical_features])\n",
        "\n",
        "    X = data.drop(columns=[\"stroke\"], axis=1)\n",
        "    Y = data[\"stroke\"]\n",
        "\n",
        "    cat_features_indices = [\n",
        "        data.columns.get_loc(label) for label in categorical_features\n",
        "    ]\n",
        "\n",
        "    smote_nc = SMOTENC(categorical_features=cat_features_indices, random_state=seed)\n",
        "    smote_een = SMOTEENN(smote=smote_nc, random_state=seed, sampling_strategy=\"auto\")\n",
        "\n",
        "    x_resampled, y_resampled = smote_een.fit_resample(X, Y)\n",
        "    x_resampled[\"stroke\"] = y_resampled\n",
        "\n",
        "    x_resampled[categorical_features] = enc.inverse_transform(\n",
        "        x_resampled[categorical_features]\n",
        "    )\n",
        "\n",
        "    return pd.DataFrame(x_resampled, columns=data.columns)\n",
        "\n",
        "\n",
        "def scale(df):\n",
        "    X_num = df[NUMERIC_FEATURE_NAMES]\n",
        "    X_cat = df[CATEGORICAL_FEATURE_NAMES]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_num)\n",
        "\n",
        "    X_scaled = scaler.transform(X_num)\n",
        "    X_scaled = pd.DataFrame(X_scaled, index=X_num.index, columns=X_num.columns)\n",
        "\n",
        "    df_scaled = pd.concat([X_scaled, X_cat, df[\"stroke\"]], axis=1)[df.columns]\n",
        "\n",
        "    return df_scaled\n",
        "\n",
        "\n",
        "def split_train_valid_test(data_df, seed: int, resample_training: bool):\n",
        "    data_df = data_df.sample(frac=1, random_state=seed)\n",
        "\n",
        "    test_set = data_df[round(len(data_df) * 0.85) :]\n",
        "    train_validation_data = data_df[: round(len(data_df) * 0.85)].sample(\n",
        "        frac=1, random_state=seed\n",
        "    )\n",
        "\n",
        "    train_set = train_validation_data[: round(len(data_df) * 0.70)]\n",
        "    validation_set = train_validation_data[round(len(data_df) * 0.70) :]\n",
        "\n",
        "    if resample_training:\n",
        "        train_set = resample(\n",
        "            train_validation_data[: round(len(data_df) * 0.70)],\n",
        "            seed,\n",
        "            categorical_features=CATEGORICAL_FEATURE_NAMES,\n",
        "        )\n",
        "\n",
        "    return train_set, validation_set, test_set\n",
        "\n",
        "\n",
        "def split_train_valid_test_stratified(data_df, seed: int, resample_training: bool):\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=seed)\n",
        "    d_x, d_y = split_label(data_df)\n",
        "\n",
        "    train_index, valid_test_index = list(sss.split(d_x, d_y))[0]\n",
        "\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
        "\n",
        "    v_t_x, v_t_y = split_label(data_df.iloc[valid_test_index])\n",
        "\n",
        "    validation_index, test_index = list(sss.split(v_t_x, v_t_y))[0]\n",
        "\n",
        "    train_df = data_df.iloc[train_index.tolist()]\n",
        "    validation_df = data_df.iloc[validation_index.tolist()]\n",
        "    test_df = data_df.iloc[test_index.tolist()]\n",
        "\n",
        "    if resample_training:\n",
        "        train_df = resample(\n",
        "            train_df, seed, categorical_features=CATEGORICAL_FEATURE_NAMES\n",
        "        )\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "\n",
        "def prepare_data(seed: int, resample_training: bool):\n",
        "    data_df = pd.read_csv(Path().resolve().joinpath(\"dataset/full_data_clean.csv\"))\n",
        "\n",
        "    train_df, validation_df, test_df = split_train_valid_test_stratified(\n",
        "        data_df, seed, resample_training\n",
        "    )\n",
        "\n",
        "    train_df, validation_df, test_df = [\n",
        "        scale(df) for df in [train_df, validation_df, test_df]\n",
        "    ]\n",
        "\n",
        "    return train_df, validation_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model hyperparameters\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "DROPOUT_RATE = 0.1\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "MLP_MODEL_PATH = str(Path().resolve().joinpath(\"model/mlp_model\"))\n",
        "TABTRANSFORMER_MODEL_PATH = str(Path().resolve().joinpath(\"model/tabtransformer_model\"))\n",
        "\n",
        "TARGET_FEATURE_NAME = \"stroke\"\n",
        "TARGET_LABELS = [\"1\", \"0\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data proccessing pipeline\n",
        "\n",
        "#target_label_lookup = layers.StringLookup(\n",
        "#    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        "#)\n",
        "\n",
        "def prepare_example(features, target):\n",
        "    # target_index = target_label_lookup(target)\n",
        "    target_index = target\n",
        "    return features, target_index\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
        "    \"\"\"dataset from, csv\"\"\"\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
        "    return dataset.cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training and evaluation\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_data_file,\n",
        "    test_data_file,\n",
        "    model_output,\n",
        "    num_epochs,\n",
        "    EPOCHS_TO_WAIT_FOR_IMPROVE,\n",
        "    learning_rate,\n",
        "    batch_size,\n",
        "):\n",
        "    \"\"\"Implement a training and evaluation procedure\"\"\"\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
        "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
        "\n",
        "    metrics = [\n",
        "            keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
        "            keras.metrics.AUC(name=\"auc\"),\n",
        "        ]\n",
        "\n",
        "    early_stop_callback = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=EPOCHS_TO_WAIT_FOR_IMPROVE\n",
        "    )\n",
        "    # checkpoint_callback = keras.callbacks.ModelCheckpoint(model_output, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "    # early_stop_callback = keras.callbacks.EarlyStopping(\n",
        "    #    monitor=\"val_auc\", patience=EPOCHS_TO_WAIT_FOR_IMPROVE\n",
        "    # )\n",
        "\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        model_output,\n",
        "        monitor=\"val_loss\",\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=validation_dataset,\n",
        "        callbacks=[\n",
        "            checkpoint_callback,\n",
        "            early_stop_callback\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    model.save(model_output)\n",
        "\n",
        "    _, accuracy, auc = model.evaluate(validation_dataset, verbose=0)\n",
        "\n",
        "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}% AUC: {auc}\")\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESULT_COLS = [\n",
        "    \"precision\",\n",
        "    \"recall\",\n",
        "    \"fscore\",\n",
        "    \"accuracy\",\n",
        "    \"auc\",\n",
        "    \"miss_rate\",\n",
        "    \"fall_out_rate\",\n",
        "]\n",
        "\n",
        "train_data_path = Path().resolve().joinpath(\"dataset/train_data.csv\")\n",
        "validation_data_path = Path().resolve().joinpath(\"dataset/validation_data.csv\")\n",
        "test_data_path = Path().resolve().joinpath(\"dataset/test_data.csv\")\n",
        "\n",
        "train_data_file = str(train_data_path.absolute())\n",
        "validation_data_file = str(validation_data_path.absolute())\n",
        "test_data_file = str(test_data_path.absolute())\n",
        "\n",
        "NUM_EXPERIMENTS = 10\n",
        "EPOCHS_TO_WAIT_FOR_IMPROVE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(\n",
        "    experiment,\n",
        "    tabtransformer_model,\n",
        "    seed,\n",
        "    train_data,\n",
        "    validation_data,\n",
        "    test_data,\n",
        "    resampled: bool,\n",
        "):\n",
        "    # split labels\n",
        "    train_data_file = str(\n",
        "        Path().resolve().joinpath(f\"dataset/train_data_tt_exp_{experiment}.csv\")\n",
        "    )\n",
        "    validation_data_file = str(\n",
        "        Path().resolve().joinpath(f\"dataset/validation_data_tt_exp_{experiment}.csv\")\n",
        "    )\n",
        "    test_data_file = str(\n",
        "        Path().resolve().joinpath(f\"dataset/test_data_tt_exp_{experiment}.csv\")\n",
        "    )\n",
        "\n",
        "    train_data.to_csv(train_data_file, header=False, index=False)\n",
        "    validation_data.to_csv(validation_data_file, header=False, index=False)\n",
        "    test_data.to_csv(test_data_file, header=False, index=False)\n",
        "\n",
        "    if resampled:\n",
        "        model_output = str(\n",
        "            Path().resolve().joinpath(f\"model/tt_model_exp_{experiment}_resampled\")\n",
        "        )\n",
        "    else:\n",
        "        model_output = str(\n",
        "            Path().resolve().joinpath(f\"model/tt_model_exp_{experiment}_unsampled\")\n",
        "        )\n",
        "\n",
        "    # train tabtransformer model on training data and evaluate on validation data\n",
        "    history, tt_trained = train(\n",
        "        model=tabtransformer_model,\n",
        "        train_data_file=train_data_file,\n",
        "        test_data_file=validation_data_file,\n",
        "        model_output=model_output,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        EPOCHS_TO_WAIT_FOR_IMPROVE=EPOCHS_TO_WAIT_FOR_IMPROVE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    train_validation_data_file = str(\n",
        "        Path()\n",
        "        .resolve()\n",
        "        .joinpath(f\"dataset/train_validation_data_tt_exp_{experiment}.csv\")\n",
        "    )\n",
        "\n",
        "    pd.concat([validation_data, train_data]).sample(frac=1, random_state=seed).to_csv(\n",
        "        train_validation_data_file, index=False, header=False\n",
        "    )\n",
        "\n",
        "    # now, train tabtransformer model on validation data and evaluate on test data\n",
        "    history, tt_trained = train(\n",
        "        model=tt_trained,\n",
        "        train_data_file=train_validation_data_file,\n",
        "        test_data_file=test_data_file,\n",
        "        model_output=model_output,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        EPOCHS_TO_WAIT_FOR_IMPROVE=EPOCHS_TO_WAIT_FOR_IMPROVE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    # cleanup\n",
        "    os.remove(train_data_file)\n",
        "    os.remove(validation_data_file)\n",
        "    os.remove(train_validation_data_file)\n",
        "    # os.remove(test_data_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-08 00:50:24.821778: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
            "2022-09-08 00:50:24.821875: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: haoming-t480s\n",
            "2022-09-08 00:50:24.821901: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: haoming-t480s\n",
            "2022-09-08 00:50:24.822395: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.65.1\n",
            "2022-09-08 00:50:24.822492: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.65.1\n",
            "2022-09-08 00:50:24.822515: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.65.1\n",
            "2022-09-08 00:50:24.823235: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Experiment 0, seed 753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_36153/1735405672.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[categorical_features] = enc.fit_transform(data[categorical_features])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/100\n",
            "    200/Unknown - 18s 47ms/step - loss: 0.2591 - acc: 0.9039 - auc: 0.9564\n",
            "Epoch 1: val_loss improved from inf to 7.11451, saving model to /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 36). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 34s 123ms/step - loss: 0.2586 - acc: 0.9041 - auc: 0.9565 - val_loss: 7.1145 - val_acc: 0.1667 - val_auc: 0.5886\n",
            "Epoch 2/100\n",
            "200/201 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8888 - auc: 0.9362\n",
            "Epoch 2: val_loss improved from 7.11451 to 4.03554, saving model to /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 36). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 30s 149ms/step - loss: 0.3244 - acc: 0.8890 - auc: 0.9363 - val_loss: 4.0355 - val_acc: 0.2771 - val_auc: 0.6760\n",
            "Epoch 3/100\n",
            "201/201 [==============================] - ETA: 0s - loss: 0.3536 - acc: 0.9002 - auc: 0.9466\n",
            "Epoch 3: val_loss improved from 4.03554 to 0.67729, saving model to /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 36). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 19s 95ms/step - loss: 0.3536 - acc: 0.9002 - auc: 0.9466 - val_loss: 0.6773 - val_acc: 0.6240 - val_auc: 0.6956\n",
            "Epoch 4/100\n",
            "200/201 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.8846 - auc: 0.9335\n",
            "Epoch 4: val_loss improved from 0.67729 to 0.09735, saving model to /home/haoming/Projects/python/brain-stroke-prediction/model/tt_model_exp_0_resampled\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 36). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "\n",
        "tabtransformer_model = keras.models.load_model(TABTRANSFORMER_MODEL_PATH)\n",
        "\n",
        "for experiment in range(NUM_EXPERIMENTS):\n",
        "    seed = random.randint(0, 1000)\n",
        "\n",
        "    print(f\"Experiment {experiment}, seed {seed}\")\n",
        "\n",
        "    \"\"\" \n",
        "    train_data, validation_data, test_data = prepare_data(\n",
        "        seed=seed, resample_training=False\n",
        "    )\n",
        "\n",
        "    run_experiment(\n",
        "        experiment=experiment,\n",
        "        tabtransformer_model=tabtransformer_model,\n",
        "        seed=seed,\n",
        "        train_data=train_data,\n",
        "        validation_data=validation_data,\n",
        "        test_data=test_data,\n",
        "        resampled=False,\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    train_data, validation_data, test_data = prepare_data(\n",
        "        seed=seed, resample_training=True\n",
        "    )\n",
        "\n",
        "    run_experiment(\n",
        "        experiment=experiment,\n",
        "        tabtransformer_model=tabtransformer_model,\n",
        "        seed=seed,\n",
        "        train_data=train_data,\n",
        "        validation_data=validation_data,\n",
        "        test_data=test_data,\n",
        "        resampled=True,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metrics_keras(model: keras.Model, test_data_file: str):\n",
        "    model.compile(\n",
        "        metrics=[\n",
        "            keras.metrics.AUC(\n",
        "                num_thresholds=200,\n",
        "                curve=\"ROC\",\n",
        "            ),\n",
        "            keras.metrics.BinaryAccuracy(),\n",
        "            keras.metrics.Precision(),\n",
        "            keras.metrics.Recall(),\n",
        "            keras.metrics.TrueNegatives(),\n",
        "            keras.metrics.FalsePositives(),\n",
        "            keras.metrics.TrueNegatives(),\n",
        "            keras.metrics.TruePositives(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    _, auc, accuracy, precision, recall, tn, fn, fp, tp = model.evaluate(\n",
        "        get_dataset_from_csv(test_data_file)\n",
        "    )\n",
        "\n",
        "    # metrics\n",
        "    fscore = 2 * tp / (2 * tp + fp + fn)\n",
        "    miss_rate = fn / (tn + tp)\n",
        "    fall_out_rate = fp / (fp + tn)\n",
        "\n",
        "    # return\n",
        "    return [precision, recall, fscore, accuracy, auc, miss_rate, fall_out_rate]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51/51 [==============================] - 2s 15ms/step - loss: 0.0000e+00 - auc_18: 0.7484 - binary_accuracy: 0.6604 - precision_18: 0.0333 - recall_18: 0.6696 - true_negatives_36: 4224.0000 - false_positives_18: 2174.0000 - true_negatives_37: 4224.0000 - true_positives_18: 75.0000\n",
            "51/51 [==============================] - 2s 14ms/step - loss: 0.0000e+00 - auc_19: 0.7379 - binary_accuracy: 0.6461 - precision_19: 0.0307 - recall_19: 0.6792 - true_negatives_38: 4134.0000 - false_positives_19: 2270.0000 - true_negatives_39: 4134.0000 - true_positives_19: 72.0000\n",
            "51/51 [==============================] - 3s 20ms/step - loss: 0.0000e+00 - auc_20: 0.7857 - binary_accuracy: 0.9819 - precision_20: 0.0000e+00 - recall_20: 0.0000e+00 - true_negatives_40: 6392.0000 - false_positives_20: 0.0000e+00 - true_negatives_41: 6392.0000 - true_positives_20: 0.0000e+00\n",
            "51/51 [==============================] - 3s 16ms/step - loss: 0.0000e+00 - auc_21: 0.7388 - binary_accuracy: 0.6581 - precision_21: 0.0357 - recall_21: 0.6864 - true_negatives_42: 4203.0000 - false_positives_21: 2189.0000 - true_negatives_43: 4203.0000 - true_positives_21: 81.0000\n",
            "51/51 [==============================] - 2s 14ms/step - loss: 0.0000e+00 - auc_22: 0.7841 - binary_accuracy: 0.9817 - precision_22: 0.0000e+00 - recall_22: 0.0000e+00 - true_negatives_44: 6391.0000 - false_positives_22: 0.0000e+00 - true_negatives_45: 6391.0000 - true_positives_22: 0.0000e+00\n",
            "51/51 [==============================] - 3s 17ms/step - loss: 0.0000e+00 - auc_23: 0.7359 - binary_accuracy: 0.6292 - precision_23: 0.0356 - recall_23: 0.7395 - true_negatives_46: 4008.0000 - false_positives_23: 2383.0000 - true_negatives_47: 4008.0000 - true_positives_23: 88.0000\n",
            "51/51 [==============================] - 4s 28ms/step - loss: 0.0000e+00 - auc_24: 0.7822 - binary_accuracy: 0.9822 - precision_24: 0.0000e+00 - recall_24: 0.0000e+00 - true_negatives_48: 6394.0000 - false_positives_24: 0.0000e+00 - true_negatives_49: 6394.0000 - true_positives_24: 0.0000e+00\n",
            "51/51 [==============================] - 5s 22ms/step - loss: 0.0000e+00 - auc_25: 0.7795 - binary_accuracy: 0.6330 - precision_25: 0.0356 - recall_25: 0.7500 - true_negatives_50: 4034.0000 - false_positives_25: 2360.0000 - true_negatives_51: 4034.0000 - true_positives_25: 87.0000\n",
            "51/51 [==============================] - 3s 15ms/step - loss: 0.0000e+00 - auc_26: 0.7694 - binary_accuracy: 0.9839 - precision_26: 0.0000e+00 - recall_26: 0.0000e+00 - true_negatives_52: 6405.0000 - false_positives_26: 0.0000e+00 - true_negatives_53: 6405.0000 - true_positives_26: 0.0000e+00\n",
            "51/51 [==============================] - 3s 29ms/step - loss: 0.0000e+00 - auc_27: 0.7295 - binary_accuracy: 0.6344 - precision_27: 0.0286 - recall_27: 0.6571 - true_negatives_54: 4061.0000 - false_positives_27: 2344.0000 - true_negatives_55: 4061.0000 - true_positives_27: 69.0000\n",
            "51/51 [==============================] - 4s 23ms/step - loss: 0.0000e+00 - auc_28: 0.7903 - binary_accuracy: 0.9840 - precision_28: 0.0000e+00 - recall_28: 0.0000e+00 - true_negatives_56: 6406.0000 - false_positives_28: 0.0000e+00 - true_negatives_57: 6406.0000 - true_positives_28: 0.0000e+00\n",
            "51/51 [==============================] - 3s 25ms/step - loss: 0.0000e+00 - auc_29: 0.7756 - binary_accuracy: 0.6336 - precision_29: 0.0324 - recall_29: 0.7596 - true_negatives_58: 4046.0000 - false_positives_29: 2360.0000 - true_negatives_59: 4046.0000 - true_positives_29: 79.0000\n",
            "51/51 [==============================] - 3s 17ms/step - loss: 0.0000e+00 - auc_30: 0.7735 - binary_accuracy: 0.9810 - precision_30: 0.0000e+00 - recall_30: 0.0000e+00 - true_negatives_60: 6386.0000 - false_positives_30: 0.0000e+00 - true_negatives_61: 6386.0000 - true_positives_30: 0.0000e+00\n",
            "51/51 [==============================] - 3s 18ms/step - loss: 0.0000e+00 - auc_31: 0.7558 - binary_accuracy: 0.5822 - precision_31: 0.0361 - recall_31: 0.8145 - true_negatives_62: 3689.0000 - false_positives_31: 2697.0000 - true_negatives_63: 3689.0000 - true_positives_31: 101.0000\n",
            "51/51 [==============================] - 3s 17ms/step - loss: 0.0000e+00 - auc_32: 0.7736 - binary_accuracy: 0.9842 - precision_32: 0.0000e+00 - recall_32: 0.0000e+00 - true_negatives_64: 6407.0000 - false_positives_32: 0.0000e+00 - true_negatives_65: 6407.0000 - true_positives_32: 0.0000e+00\n",
            "51/51 [==============================] - 3s 19ms/step - loss: 0.0000e+00 - auc_33: 0.7401 - binary_accuracy: 0.6782 - precision_33: 0.0337 - recall_33: 0.6990 - true_negatives_66: 4343.0000 - false_positives_33: 2064.0000 - true_negatives_67: 4343.0000 - true_positives_33: 72.0000\n",
            "51/51 [==============================] - 3s 16ms/step - loss: 0.0000e+00 - auc_34: 0.7946 - binary_accuracy: 0.9828 - precision_34: 0.0000e+00 - recall_34: 0.0000e+00 - true_negatives_68: 6398.0000 - false_positives_34: 0.0000e+00 - true_negatives_69: 6398.0000 - true_positives_34: 0.0000e+00\n",
            "51/51 [==============================] - 3s 17ms/step - loss: 0.0000e+00 - auc_35: 0.7944 - binary_accuracy: 0.9837 - precision_35: 0.0000e+00 - recall_35: 0.0000e+00 - true_negatives_70: 6404.0000 - false_positives_35: 0.0000e+00 - true_negatives_71: 6404.0000 - true_positives_35: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# evaluate unsampled\n",
        "results = {}\n",
        "results_resampling = {}\n",
        "\n",
        "test_data = {}\n",
        "\n",
        "\n",
        "for file in Path().resolve().joinpath(\"dataset/\").iterdir():\n",
        "    if str(file.name).startswith(\"test_data_tt_exp_\"):\n",
        "        exp_num = file.name.split(\"_\")[4][0]\n",
        "\n",
        "        test_data[exp_num] = str(file)\n",
        "\n",
        "for file in Path().resolve().joinpath(\"model/\").iterdir():\n",
        "    if str(file).endswith(\"_unsampled\") and str(file.name).startswith(\"tt_model_exp_\"):\n",
        "        exp_num = file.name.split(\"_\")[3]\n",
        "\n",
        "        model = tf.keras.models.load_model(file)\n",
        "\n",
        "        results[exp_num] = list(metrics_keras(model, test_data[exp_num]))\n",
        "\n",
        "    if str(file).endswith(\"_resampled\") and str(file.name).startswith(\"tt_model_exp_\"):\n",
        "        exp_num = file.name.split(\"_\")[3]\n",
        "\n",
        "        model = tf.keras.models.load_model(file)\n",
        "\n",
        "        results_resampling[exp_num] = list(metrics_keras(model, test_data[exp_num]))\n",
        "\n",
        "\n",
        "results = pd.DataFrame().from_dict(results, orient=\"index\", columns=RESULT_COLS)\n",
        "results_resampling = pd.DataFrame().from_dict(\n",
        "    results_resampling, orient=\"index\", columns=RESULT_COLS\n",
        ")\n",
        "\n",
        "results[\"classifier\"] = \"TabTransformer\"\n",
        "results_resampling[\"classifier\"] = \"TabTransformer\"\n",
        "\n",
        "results.to_csv(Path().resolve().joinpath(\"results/TabTransformer_eval.csv\"))\n",
        "results_resampling.to_csv(\n",
        "    Path().resolve().joinpath(\"results/TabTransformer_eval_resampled.csv\")\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5d9c7b03d18c7708763fb0db9310b2d7254dfc13fc51ca537e4e50c45d8de958"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
