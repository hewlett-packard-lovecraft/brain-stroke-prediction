{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 Physical GPUs, 1 Logical GPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-06 13:05:47.544415: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-09-06 13:05:49.562626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3371 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:09:00.0, compute capability: 6.1\n"
          ]
        }
      ],
      "source": [
        "import keras_preprocessing\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "gpu_number = 0  #### GPU number\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    tf.config.experimental.set_visible_devices(gpus[gpu_number], \"GPU\")\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import sklearn\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import SMOTENC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_HEADER = [\n",
        "    \"gender\",\n",
        "    \"age\",\n",
        "    \"hypertension\",\n",
        "    \"heart_disease\",\n",
        "    \"ever_married\",\n",
        "    \"work_type\",\n",
        "    \"residence_type\",\n",
        "    \"avg_glucose_level\",\n",
        "    \"bmi\",\n",
        "    \"smoking_status\",\n",
        "    \"stroke\",\n",
        "]\n",
        "\n",
        "\n",
        "CATEGORICAL_FEATURE_NAMES = [\n",
        "    \"gender\",\n",
        "    \"hypertension\",\n",
        "    \"heart_disease\",\n",
        "    \"ever_married\",\n",
        "    \"work_type\",\n",
        "    \"residence_type\",\n",
        "    \"smoking_status\",\n",
        "]\n",
        "\n",
        "NUMERIC_FEATURE_NAMES = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
        "\n",
        "\n",
        "def encode_int(data: pd.DataFrame, categorical_features: list[str]):\n",
        "    return pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "\n",
        "def split_label(data: pd.DataFrame):\n",
        "    x = data.copy().drop(\"stroke\", axis=1)\n",
        "    y = data[\"stroke\"]  # labels\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def resample(data: pd.DataFrame, seed: int, categorical_features: list[str]):\n",
        "    \"\"\"oversample positive cases with SMOTE and undersample negative with EEN\"\"\"\n",
        "    # encode categorical features first\n",
        "    enc = OrdinalEncoder()\n",
        "    data[categorical_features] = enc.fit_transform(data[categorical_features])\n",
        "\n",
        "    X = data.drop(columns=[\"stroke\"], axis=1)\n",
        "    Y = data[\"stroke\"]\n",
        "\n",
        "    cat_features_indices = [\n",
        "        data.columns.get_loc(label) for label in categorical_features\n",
        "    ]\n",
        "\n",
        "    smote_nc = SMOTENC(categorical_features=cat_features_indices, random_state=seed)\n",
        "    smote_een = SMOTEENN(smote=smote_nc, random_state=seed, sampling_strategy=\"auto\")\n",
        "\n",
        "    x_resampled, y_resampled = smote_een.fit_resample(X, Y)\n",
        "    x_resampled[\"stroke\"] = y_resampled\n",
        "\n",
        "    x_resampled[categorical_features] = enc.inverse_transform(\n",
        "        x_resampled[categorical_features]\n",
        "    )\n",
        "\n",
        "    return pd.DataFrame(x_resampled, columns=data.columns)\n",
        "\n",
        "\n",
        "def scale(df):\n",
        "    X_num = df[NUMERIC_FEATURE_NAMES]\n",
        "    X_cat = df[CATEGORICAL_FEATURE_NAMES]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_num)\n",
        "\n",
        "    X_scaled = scaler.transform(X_num)\n",
        "    X_scaled = pd.DataFrame(X_scaled, index=X_num.index, columns=X_num.columns)\n",
        "\n",
        "    df_scaled = pd.concat([X_scaled, X_cat, df[\"stroke\"]], axis=1)[df.columns]\n",
        "\n",
        "    return df_scaled\n",
        "\n",
        "\n",
        "def split_train_valid_test(data_df, seed: int, resample_training: bool):\n",
        "    data_df = data_df.sample(frac=1, random_state=seed)\n",
        "\n",
        "    test_set = data_df[round(len(data_df) * 0.85) :]\n",
        "    train_validation_data = data_df[: round(len(data_df) * 0.85)].sample(\n",
        "        frac=1, random_state=seed\n",
        "    )\n",
        "\n",
        "    train_set = train_validation_data[: round(len(data_df) * 0.70)]\n",
        "    validation_set = train_validation_data[round(len(data_df) * 0.70) :]\n",
        "\n",
        "    if resample_training:\n",
        "        train_set = resample(\n",
        "            train_validation_data[: round(len(data_df) * 0.70)],\n",
        "            seed,\n",
        "            categorical_features=CATEGORICAL_FEATURE_NAMES,\n",
        "        )\n",
        "\n",
        "    return train_set, validation_set, test_set\n",
        "\n",
        "\n",
        "def split_train_valid_test_stratified(data_df, seed: int, resample_training: bool):\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=seed)\n",
        "    d_x, d_y = split_label(data_df)\n",
        "\n",
        "    train_index, valid_test_index = list(sss.split(d_x, d_y))[0]\n",
        "\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
        "\n",
        "    v_t_x, v_t_y = split_label(data_df.iloc[valid_test_index])\n",
        "\n",
        "    validation_index, test_index = list(sss.split(v_t_x, v_t_y))[0]\n",
        "\n",
        "    train_df = data_df.iloc[train_index.tolist()]\n",
        "    validation_df = data_df.iloc[validation_index.tolist()]\n",
        "    test_df = data_df.iloc[test_index.tolist()]\n",
        "\n",
        "    if resample_training:\n",
        "        train_set = resample(\n",
        "            train_df, seed, categorical_features=CATEGORICAL_FEATURE_NAMES\n",
        "        )\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data(seed: int, resample_training: bool):\n",
        "    data_df = pd.read_csv(Path().resolve().joinpath(\"dataset/full_data_clean.csv\"))\n",
        "\n",
        "    train_df, validation_df, test_df = split_train_valid_test_stratified(\n",
        "        data_df, seed, resample_training\n",
        "    )\n",
        "\n",
        "    train_df, validation_df, test_df = [\n",
        "        scale(df) for df in [train_df, validation_df, test_df]\n",
        "    ]\n",
        "\n",
        "    return train_df, validation_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "DROPOUT_RATE = 0.1\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "MLP_MODEL_PATH = str(Path().resolve().joinpath(\"model/mlp_model\"))\n",
        "TABTRANSFORMER_MODEL_PATH = str(Path().resolve().joinpath(\"model/tabtransformer_model\"))\n",
        "\n",
        "TARGET_FEATURE_NAME = \"stroke\"\n",
        "TARGET_LABELS = [\"1\", \"0\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/haoming/.local/lib/python3.10/site-packages/numpy/core/numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  return bool(asarray(a1 == a2).all())\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<CacheDataset element_spec=({'age': TensorSpec(shape=(), dtype=tf.float32, name=None), 'avg_glucose_level': TensorSpec(shape=(), dtype=tf.float32, name=None), 'bmi': TensorSpec(shape=(), dtype=tf.float32, name=None), 'gender': TensorSpec(shape=(), dtype=tf.string, name=None), 'hypertension': TensorSpec(shape=(), dtype=tf.string, name=None), 'heart_disease': TensorSpec(shape=(), dtype=tf.string, name=None), 'ever_married': TensorSpec(shape=(), dtype=tf.string, name=None), 'work_type': TensorSpec(shape=(), dtype=tf.string, name=None), 'residence_type': TensorSpec(shape=(), dtype=tf.string, name=None), 'smoking_status': TensorSpec(shape=(), dtype=tf.string, name=None)}, TensorSpec(shape=(), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data proccessing pipeline\n",
        "\n",
        "target_label_lookup = layers.StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "\n",
        "\n",
        "def prepare_example(features, target):\n",
        "    # target_index = target_label_lookup(target)\n",
        "    target_index = target\n",
        "    return features, target_index\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
        "    \"\"\"dataset from, csv\"\"\"\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
        "    return dataset.cache()\n",
        "\n",
        "\n",
        "def get_dataset_from_df(df, batch_size=128, shuffle=False):\n",
        "    \"\"\"dataset from, csv\"\"\"\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            df[NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES].to_dict(\n",
        "                orient=\"list\"\n",
        "            ),\n",
        "            tf.constant(df.loc[:, TARGET_FEATURE_NAME]),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return dataset.cache()\n",
        "\n",
        "\n",
        "get_dataset_from_df(pd.read_csv(\"dataset/full_data_clean.csv\"))\n",
        "\n",
        "# tf.convert_to_tensor(pd.read_csv(\"dataset/full_data_clean.csv\").loc[:, TARGET_FEATURE_NAME])\n",
        "# get_dataset_from_csv(\"dataset/full_data_clean.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training and evaluation\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: keras.Model,\n",
        "    train_data_file,\n",
        "    test_data_file,\n",
        "    model_output,\n",
        "    num_epochs,\n",
        "    EPOCHS_TO_WAIT_FOR_IMPROVE,\n",
        "    learning_rate,\n",
        "    batch_size,\n",
        "):\n",
        "    \"\"\"Implement a training and evaluation procedure\"\"\"\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
        "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
        "\n",
        "    metrics = (\n",
        "        [\n",
        "            keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
        "            keras.metrics.AUC(name=\"auc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    early_stop_callback = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=EPOCHS_TO_WAIT_FOR_IMPROVE\n",
        "    )\n",
        "    # checkpoint_callback = keras.callbacks.ModelCheckpoint(model_output, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "    # early_stop_callback = keras.callbacks.EarlyStopping(\n",
        "    #    monitor=\"val_auc\", patience=EPOCHS_TO_WAIT_FOR_IMPROVE\n",
        "    # )\n",
        "\n",
        "    # checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    #    model_output,\n",
        "    #    monitor=\"val_auc\",\n",
        "    #    verbose=1,\n",
        "    #    save_best_only=True,\n",
        "    #    mode=\"max\",\n",
        "    # )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=validation_dataset,\n",
        "        callbacks=[\n",
        "            # checkpoint_callback,\n",
        "            early_stop_callback\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    _, accuracy, auc = model.evaluate(validation_dataset, verbose=0)\n",
        "\n",
        "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}% AUC: {auc}\")\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESULT_COLS = [\n",
        "    \"classifier\" \"precision\",\n",
        "    \"recall\",\n",
        "    \"fscore\",\n",
        "    \"accuracy\",\n",
        "    \"auc\",\n",
        "    \"miss_rate\",\n",
        "    \"fall_out_rate\",\n",
        "]\n",
        "\n",
        "train_data_path = Path().resolve().joinpath(\"dataset/train_data.csv\")\n",
        "validation_data_path = Path().resolve().joinpath(\"dataset/validation_data.csv\")\n",
        "test_data_path = Path().resolve().joinpath(\"dataset/test_data.csv\")\n",
        "\n",
        "train_data_file = str(train_data_path.absolute())\n",
        "validation_data_file = str(validation_data_path.absolute())\n",
        "test_data_file = str(test_data_path.absolute())\n",
        "\n",
        "NUM_EXPERIMENTS = 10\n",
        "EPOCHS_TO_WAIT_FOR_IMPROVE = 15\n",
        "\n",
        "\n",
        "def metrics_keras(model: keras.Model, test_data_file: str):\n",
        "    model.compile(\n",
        "        metrics=[\n",
        "            keras.metrics.AUC(\n",
        "                num_thresholds=200,\n",
        "                curve=\"ROC\",\n",
        "            ),\n",
        "            keras.metrics.Precision(),\n",
        "            keras.metrics.Recall(),\n",
        "            keras.metrics.TrueNegatives(),\n",
        "            keras.metrics.FalsePositives(),\n",
        "            keras.metrics.TrueNegatives(),\n",
        "            keras.metrics.TruePositives(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    _, auc, precision, recall, tn, fn, fp, tp = model.evaluate(\n",
        "        get_dataset_from_csv(test_data_file)\n",
        "    )\n",
        "\n",
        "    # metrics\n",
        "    fscore = 2 * tp / (2 * tp + fp + fn)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    miss_rate = fn / (tn + tp)\n",
        "    fall_out_rate = fp / (fp + tn)\n",
        "\n",
        "    # return\n",
        "    return [precision, recall, fscore, accuracy, auc, miss_rate, fall_out_rate]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(experiment, mlp_model, seed, train_data, validation_data, test_data):\n",
        "    # split labels\n",
        "    train_data_path = str(\n",
        "        Path().resolve().joinpath(f\"dataset/train_data_mlp_exp_{experiment}.csv\")\n",
        "    )\n",
        "    validation_data_path = str(\n",
        "        Path().resolve().joinpath(f\"dataset/validation_data_mlp_exp_{experiment}.csv\")\n",
        "    )\n",
        "    test_data_path = str(\n",
        "        Path().resolve().joinpath(f\"dataset/test_data_mlp_exp_{experiment}.csv\")\n",
        "    )\n",
        "\n",
        "    train_data.to_csv(train_data_path, header=False, index=False)\n",
        "    validation_data.to_csv(validation_data_path, header=False, index=False)\n",
        "    test_data.to_csv(test_data_path, header=False, index=False)\n",
        "\n",
        "    x_test, y_test = split_label(test_data)\n",
        "\n",
        "    # train tabtransformer model on training data and evaluate on validation data\n",
        "    history, mlp_trained = train(\n",
        "        model=mlp_model,\n",
        "        train_data_file=train_data_path,\n",
        "        test_data_file=validation_data_path,\n",
        "        model_output=MLP_MODEL_PATH,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        EPOCHS_TO_WAIT_FOR_IMPROVE=EPOCHS_TO_WAIT_FOR_IMPROVE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    train_validation_data_path = str(\n",
        "        Path()\n",
        "        .resolve()\n",
        "        .joinpath(f\"dataset/train_validation_data_mlp_exp_{experiment}.csv\")\n",
        "    )\n",
        "\n",
        "    pd.concat([train_data, validation_data]).sample(frac=1, random_state=seed).to_csv(\n",
        "        train_validation_data_path, index=False, header=False\n",
        "    )\n",
        "\n",
        "    # now, train tabtransformer model on validation data and evaluate on test data\n",
        "    history, mlp_trained = train(\n",
        "        model=mlp_trained,\n",
        "        train_data_file=train_validation_data_path,\n",
        "        test_data_file=test_data_path,\n",
        "        model_output=MLP_MODEL_PATH,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        EPOCHS_TO_WAIT_FOR_IMPROVE=EPOCHS_TO_WAIT_FOR_IMPROVE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    metrics = metrics_keras(model=mlp_trained, test_data_file=test_data_file)\n",
        "\n",
        "    # cleanup\n",
        "    os.remove(train_data_path)\n",
        "    os.remove(validation_data_path)\n",
        "    os.remove(train_validation_data_path)\n",
        "    os.remove(test_data_path)\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Start training the model...\n",
            "Epoch 1/100\n",
            "950/950 [==============================] - 31s 25ms/step - loss: 0.0832 - acc: 0.9814 - auc: 0.7675 - val_loss: 0.0836 - val_acc: 0.9813 - val_auc: 0.7861\n",
            "Epoch 2/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0800 - acc: 0.9822 - auc: 0.7947 - val_loss: 0.0823 - val_acc: 0.9813 - val_auc: 0.8056\n",
            "Epoch 3/100\n",
            "950/950 [==============================] - 23s 24ms/step - loss: 0.0792 - acc: 0.9822 - auc: 0.7981 - val_loss: 0.0811 - val_acc: 0.9813 - val_auc: 0.8196\n",
            "Epoch 4/100\n",
            "950/950 [==============================] - 22s 23ms/step - loss: 0.0779 - acc: 0.9822 - auc: 0.8098 - val_loss: 0.0796 - val_acc: 0.9813 - val_auc: 0.8273\n",
            "Epoch 5/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0772 - acc: 0.9822 - auc: 0.8140 - val_loss: 0.0813 - val_acc: 0.9813 - val_auc: 0.8282\n",
            "Epoch 6/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0768 - acc: 0.9822 - auc: 0.8177 - val_loss: 0.0799 - val_acc: 0.9813 - val_auc: 0.8279\n",
            "Epoch 7/100\n",
            "950/950 [==============================] - 25s 26ms/step - loss: 0.0764 - acc: 0.9822 - auc: 0.8208 - val_loss: 0.0805 - val_acc: 0.9813 - val_auc: 0.8306\n",
            "Epoch 8/100\n",
            "950/950 [==============================] - 22s 23ms/step - loss: 0.0766 - acc: 0.9822 - auc: 0.8182 - val_loss: 0.0797 - val_acc: 0.9813 - val_auc: 0.8346\n",
            "Epoch 9/100\n",
            "950/950 [==============================] - 35s 37ms/step - loss: 0.0763 - acc: 0.9822 - auc: 0.8163 - val_loss: 0.0789 - val_acc: 0.9813 - val_auc: 0.8413\n",
            "Epoch 10/100\n",
            "950/950 [==============================] - 23s 24ms/step - loss: 0.0758 - acc: 0.9822 - auc: 0.8271 - val_loss: 0.0790 - val_acc: 0.9813 - val_auc: 0.8409\n",
            "Epoch 11/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0760 - acc: 0.9822 - auc: 0.8216 - val_loss: 0.0790 - val_acc: 0.9813 - val_auc: 0.8389\n",
            "Epoch 12/100\n",
            "950/950 [==============================] - 22s 23ms/step - loss: 0.0760 - acc: 0.9822 - auc: 0.8262 - val_loss: 0.0787 - val_acc: 0.9813 - val_auc: 0.8418\n",
            "Epoch 13/100\n",
            "950/950 [==============================] - 23s 24ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8278 - val_loss: 0.0791 - val_acc: 0.9813 - val_auc: 0.8396\n",
            "Epoch 14/100\n",
            "950/950 [==============================] - 18s 19ms/step - loss: 0.0755 - acc: 0.9822 - auc: 0.8290 - val_loss: 0.0785 - val_acc: 0.9813 - val_auc: 0.8428\n",
            "Epoch 15/100\n",
            "950/950 [==============================] - 18s 19ms/step - loss: 0.0752 - acc: 0.9822 - auc: 0.8292 - val_loss: 0.0796 - val_acc: 0.9813 - val_auc: 0.8357\n",
            "Epoch 16/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0754 - acc: 0.9822 - auc: 0.8257 - val_loss: 0.0789 - val_acc: 0.9813 - val_auc: 0.8389\n",
            "Epoch 17/100\n",
            "950/950 [==============================] - 18s 19ms/step - loss: 0.0754 - acc: 0.9822 - auc: 0.8286 - val_loss: 0.0787 - val_acc: 0.9813 - val_auc: 0.8383\n",
            "Epoch 18/100\n",
            "950/950 [==============================] - 18s 19ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8302 - val_loss: 0.0793 - val_acc: 0.9813 - val_auc: 0.8370\n",
            "Epoch 19/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0758 - acc: 0.9822 - auc: 0.8251 - val_loss: 0.0783 - val_acc: 0.9813 - val_auc: 0.8393\n",
            "Epoch 20/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0749 - acc: 0.9822 - auc: 0.8309 - val_loss: 0.0786 - val_acc: 0.9813 - val_auc: 0.8413\n",
            "Epoch 21/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8288 - val_loss: 0.0781 - val_acc: 0.9813 - val_auc: 0.8443\n",
            "Epoch 22/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0750 - acc: 0.9822 - auc: 0.8297 - val_loss: 0.0788 - val_acc: 0.9813 - val_auc: 0.8407\n",
            "Epoch 23/100\n",
            "950/950 [==============================] - 24s 25ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8305 - val_loss: 0.0791 - val_acc: 0.9813 - val_auc: 0.8377\n",
            "Epoch 24/100\n",
            "950/950 [==============================] - 30s 31ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8316 - val_loss: 0.0790 - val_acc: 0.9813 - val_auc: 0.8358\n",
            "Epoch 25/100\n",
            "950/950 [==============================] - 20s 22ms/step - loss: 0.0749 - acc: 0.9822 - auc: 0.8326 - val_loss: 0.0789 - val_acc: 0.9813 - val_auc: 0.8387\n",
            "Epoch 26/100\n",
            "950/950 [==============================] - 22s 23ms/step - loss: 0.0752 - acc: 0.9822 - auc: 0.8299 - val_loss: 0.0790 - val_acc: 0.9813 - val_auc: 0.8350\n",
            "Epoch 27/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0750 - acc: 0.9822 - auc: 0.8323 - val_loss: 0.0784 - val_acc: 0.9813 - val_auc: 0.8457\n",
            "Epoch 28/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0747 - acc: 0.9822 - auc: 0.8347 - val_loss: 0.0786 - val_acc: 0.9813 - val_auc: 0.8340\n",
            "Epoch 29/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0746 - acc: 0.9822 - auc: 0.8335 - val_loss: 0.0784 - val_acc: 0.9813 - val_auc: 0.8416\n",
            "Epoch 30/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0748 - acc: 0.9822 - auc: 0.8335 - val_loss: 0.0786 - val_acc: 0.9813 - val_auc: 0.8403\n",
            "Epoch 31/100\n",
            "950/950 [==============================] - 19s 21ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8300 - val_loss: 0.0790 - val_acc: 0.9813 - val_auc: 0.8349\n",
            "Epoch 32/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0744 - acc: 0.9822 - auc: 0.8364 - val_loss: 0.0781 - val_acc: 0.9813 - val_auc: 0.8468\n",
            "Epoch 33/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0747 - acc: 0.9822 - auc: 0.8315 - val_loss: 0.0785 - val_acc: 0.9813 - val_auc: 0.8400\n",
            "Epoch 34/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0748 - acc: 0.9822 - auc: 0.8315 - val_loss: 0.0779 - val_acc: 0.9813 - val_auc: 0.8425\n",
            "Epoch 35/100\n",
            "950/950 [==============================] - 18s 19ms/step - loss: 0.0752 - acc: 0.9822 - auc: 0.8296 - val_loss: 0.0775 - val_acc: 0.9813 - val_auc: 0.8480\n",
            "Epoch 36/100\n",
            "950/950 [==============================] - 29s 30ms/step - loss: 0.0747 - acc: 0.9822 - auc: 0.8332 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8422\n",
            "Epoch 37/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0754 - acc: 0.9822 - auc: 0.8292 - val_loss: 0.0778 - val_acc: 0.9813 - val_auc: 0.8509\n",
            "Epoch 38/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0748 - acc: 0.9822 - auc: 0.8345 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8475\n",
            "Epoch 39/100\n",
            "950/950 [==============================] - 22s 23ms/step - loss: 0.0746 - acc: 0.9822 - auc: 0.8359 - val_loss: 0.0787 - val_acc: 0.9813 - val_auc: 0.8450\n",
            "Epoch 40/100\n",
            "950/950 [==============================] - 30s 31ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8280 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8462\n",
            "Epoch 41/100\n",
            "950/950 [==============================] - 28s 29ms/step - loss: 0.0746 - acc: 0.9822 - auc: 0.8349 - val_loss: 0.0781 - val_acc: 0.9813 - val_auc: 0.8471\n",
            "Epoch 42/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0747 - acc: 0.9822 - auc: 0.8351 - val_loss: 0.0784 - val_acc: 0.9813 - val_auc: 0.8439\n",
            "Epoch 43/100\n",
            "950/950 [==============================] - 17s 18ms/step - loss: 0.0747 - acc: 0.9822 - auc: 0.8362 - val_loss: 0.0785 - val_acc: 0.9813 - val_auc: 0.8387\n",
            "Epoch 44/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0748 - acc: 0.9822 - auc: 0.8312 - val_loss: 0.0785 - val_acc: 0.9813 - val_auc: 0.8452\n",
            "Epoch 45/100\n",
            "950/950 [==============================] - 23s 24ms/step - loss: 0.0747 - acc: 0.9822 - auc: 0.8343 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8430\n",
            "Epoch 46/100\n",
            "950/950 [==============================] - 25s 26ms/step - loss: 0.0755 - acc: 0.9822 - auc: 0.8257 - val_loss: 0.0777 - val_acc: 0.9813 - val_auc: 0.8453\n",
            "Epoch 47/100\n",
            "950/950 [==============================] - 24s 25ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8305 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8491\n",
            "Epoch 48/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8327 - val_loss: 0.0775 - val_acc: 0.9813 - val_auc: 0.8478\n",
            "Epoch 49/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8310 - val_loss: 0.0780 - val_acc: 0.9813 - val_auc: 0.8470\n",
            "Epoch 50/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8293 - val_loss: 0.0779 - val_acc: 0.9813 - val_auc: 0.8480\n",
            "Epoch 51/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0756 - acc: 0.9822 - auc: 0.8270 - val_loss: 0.0779 - val_acc: 0.9813 - val_auc: 0.8461\n",
            "Epoch 52/100\n",
            "950/950 [==============================] - 19s 20ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8276 - val_loss: 0.0784 - val_acc: 0.9813 - val_auc: 0.8468\n",
            "Epoch 53/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0755 - acc: 0.9822 - auc: 0.8279 - val_loss: 0.0778 - val_acc: 0.9813 - val_auc: 0.8440\n",
            "Epoch 54/100\n",
            "950/950 [==============================] - 22s 24ms/step - loss: 0.0754 - acc: 0.9822 - auc: 0.8279 - val_loss: 0.0785 - val_acc: 0.9813 - val_auc: 0.8436\n",
            "Epoch 55/100\n",
            "950/950 [==============================] - 20s 21ms/step - loss: 0.0755 - acc: 0.9822 - auc: 0.8284 - val_loss: 0.0781 - val_acc: 0.9813 - val_auc: 0.8411\n",
            "Epoch 56/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8312 - val_loss: 0.0783 - val_acc: 0.9813 - val_auc: 0.8445\n",
            "Epoch 57/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0752 - acc: 0.9822 - auc: 0.8284 - val_loss: 0.0772 - val_acc: 0.9813 - val_auc: 0.8484\n",
            "Epoch 58/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0756 - acc: 0.9822 - auc: 0.8270 - val_loss: 0.0778 - val_acc: 0.9813 - val_auc: 0.8457\n",
            "Epoch 59/100\n",
            "950/950 [==============================] - 22s 23ms/step - loss: 0.0753 - acc: 0.9822 - auc: 0.8317 - val_loss: 0.0775 - val_acc: 0.9813 - val_auc: 0.8459\n",
            "Epoch 60/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0750 - acc: 0.9822 - auc: 0.8319 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8356\n",
            "Epoch 61/100\n",
            "950/950 [==============================] - 21s 22ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8309 - val_loss: 0.0781 - val_acc: 0.9813 - val_auc: 0.8430\n",
            "Epoch 62/100\n",
            "950/950 [==============================] - 25s 26ms/step - loss: 0.0751 - acc: 0.9822 - auc: 0.8302 - val_loss: 0.0785 - val_acc: 0.9813 - val_auc: 0.8436\n",
            "Epoch 63/100\n",
            "950/950 [==============================] - 24s 25ms/step - loss: 0.0756 - acc: 0.9822 - auc: 0.8254 - val_loss: 0.0782 - val_acc: 0.9813 - val_auc: 0.8448\n",
            "Epoch 64/100\n",
            "950/950 [==============================] - 25s 27ms/step - loss: 0.0752 - acc: 0.9822 - auc: 0.8315 - val_loss: 0.0778 - val_acc: 0.9813 - val_auc: 0.8440\n",
            "Epoch 65/100\n",
            "932/950 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9822 - auc: 0.8265"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m seed \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_data, validation_data, test_data \u001b[39m=\u001b[39m prepare_data(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     seed\u001b[39m=\u001b[39mseed, resample_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m results[experiment] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     run_experiment(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         experiment\u001b[39m=\u001b[39;49mexperiment,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         mlp_model\u001b[39m=\u001b[39;49mmlp_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         train_data\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         test_data\u001b[39m=\u001b[39;49mtest_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_data, validation_data, test_data \u001b[39m=\u001b[39m prepare_data(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     seed\u001b[39m=\u001b[39mseed, resample_training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m results_resampling[experiment] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     run_experiment(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         experiment\u001b[39m=\u001b[39mexperiment,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m )\n",
            "\u001b[1;32m/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb Cell 9\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(experiment, mlp_model, seed, train_data, validation_data, test_data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x_test, y_test \u001b[39m=\u001b[39m split_label(test_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# train tabtransformer model on training data and evaluate on validation data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m history, mlp_trained \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmlp_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     train_data_file\u001b[39m=\u001b[39;49mtrain_data_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     test_data_file\u001b[39m=\u001b[39;49mvalidation_data_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     model_output\u001b[39m=\u001b[39;49mMLP_MODEL_PATH,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     EPOCHS_TO_WAIT_FOR_IMPROVE\u001b[39m=\u001b[39;49mEPOCHS_TO_WAIT_FOR_IMPROVE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mLEARNING_RATE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m train_validation_data_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     Path()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m.\u001b[39mresolve()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m.\u001b[39mjoinpath(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdataset/train_validation_data_mlp_exp_\u001b[39m\u001b[39m{\u001b[39;00mexperiment\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m pd\u001b[39m.\u001b[39mconcat([train_data, validation_data])\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39mseed)\u001b[39m.\u001b[39mto_csv(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     train_validation_data_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m )\n",
            "\u001b[1;32m/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_file, test_data_file, model_output, num_epochs, EPOCHS_TO_WAIT_FOR_IMPROVE, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     metrics\u001b[39m=\u001b[39mmetrics,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart training the model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     train_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39m# checkpoint_callback,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         early_stop_callback\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel training finished\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haoming/Projects/python/brain-stroke-prediction/eval_mlp.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m _, accuracy, auc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(validation_dataset, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mlp_model = keras.models.load_model(MLP_MODEL_PATH)\n",
        "\n",
        "results = {}\n",
        "results_resampling = {}\n",
        "\n",
        "for experiment in range(NUM_EXPERIMENTS):\n",
        "    seed = random.randint(0, 1000)\n",
        "\n",
        "    train_data, validation_data, test_data = prepare_data(\n",
        "        seed=seed, resample_training=False\n",
        "    )\n",
        "\n",
        "    results[experiment] = list(\n",
        "        run_experiment(\n",
        "            experiment=experiment,\n",
        "            mlp_model=mlp_model,\n",
        "            seed=seed,\n",
        "            train_data=train_data,\n",
        "            validation_data=validation_data,\n",
        "            test_data=test_data,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    train_data, validation_data, test_data = prepare_data(\n",
        "        seed=seed, resample_training=True\n",
        "    )\n",
        "\n",
        "    results_resampling[experiment] = list(\n",
        "        run_experiment(\n",
        "            experiment=experiment,\n",
        "            mlp_model=mlp_model,\n",
        "            seed=seed,\n",
        "            train_data=train_data,\n",
        "            validation_data=validation_data,\n",
        "            test_data=test_data,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "results = pd.DataFrame().from_dict(results, orient=\"index\", columns=RESULT_COLS)\n",
        "results_resampling = pd.DataFrame().from_dict(\n",
        "    results_resampling, orient=\"index\", columns=RESULT_COLS\n",
        ")\n",
        "\n",
        "results[\"classifier\"] = \"MLP\"\n",
        "results_resampling[\"classifier\"] = \"MLP\"\n",
        "\n",
        "results.to_csv(Path().resolve().joinpath(\"results/MLP_eval.csv\"))\n",
        "results_resampling.to_csv(Path().resolve().joinpath(\"results/MLP_eval_resampled.csv\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_resampling\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5d9c7b03d18c7708763fb0db9310b2d7254dfc13fc51ca537e4e50c45d8de958"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
